
<!DOCTYPE html>
<body>

<head>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"
          integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <link href='http://fonts.googleapis.com/css?family=Lato:300,400,900' rel='stylesheet' type='text/css'>
    <link href="style.css" rel="stylesheet">
    <meta charset="utf-8">

	<title>AFL-Net</title>
	<link href="css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">

    <style>
        table {
      border-collapse: collapse;
      width: 100%;
    }

    td, th {
        border: 1px solid #777;
        text-align: center;
        padding: 8px;
        border-left:0px;
        border-right:0px;
    }

    .video-container {
      height: 100px; /* 设置视频容器的固定高度 */
      padding-bottom: 56.25%;
    }

    .video-container video {
      width: 100%;
      height: 100%;
    }


    .video-container .play-icon {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      font-size: 48px;
      color: #fff;
      cursor: pointer;
      opacity: 0.7;
    }

  </style>

    <script>
 const leftArrow = document.querySelector('.left-arrow');
const rightArrow = document.querySelector('.right-arrow');
const parentContainer = document.querySelector('.parent-container');

let currentIndex = 0;

leftArrow.addEventListener('click', scrollLeft);
rightArrow.addEventListener('click', scrollRight);

function scrollLeft() {
  if (currentIndex > 0) {
    currentIndex--;
    scrollToCurrentIndex();
  }
}

function scrollRight() {
  if (currentIndex < parentContainer.childElementCount - 1) {
    currentIndex++;
    scrollToCurrentIndex();
  }
}

function scrollToCurrentIndex() {
  const offset = currentIndex * parentContainer.offsetWidth;
  parentContainer.scrollTo({
    left: offset,
    behavior: 'smooth'
  });
}
        function playVideo(video) {
          video.play();
        }

        function pauseVideo(video) {
          video.pause();
        }
  </script>
</head>

<body data-new-gr-c-s-check-loaded="14.1091.0" data-gr-ext-installed="">


<div class="container" >
<header role="banner">
</header>
<main role="main">
<article itemscope itemtype="https://schema.org/BlogPosting">

<div class="container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded">
    <div class="column has-text-centered">
    <h1 class="title is-1 publication-title"> <b>AFL-Net:</b>  </h1>
    <h3 class="title is-3 publication-title"> INTEGRATING AUDIO, FACIAL, AND LIP MODALITIES WITH </h3>
    <h2 class="title is-3 publication-title"> CROSS-ATTENTION FOR ROBUST SPEAKER DIARIZATION IN THE WILD </h2>
    </div>
    <br>
    <div class="is-size-5">
        <span class="author-block">
          <h4 style="text-align: center;"><b>Authors: </b>Yongkang Yin<sup>1</sup><sup>,</sup><sup>2</sup>, Xu Li<sup>2</sup><sup>,</sup><sup>*</sup>, Ying Shan<sup>2</sup>, Yuexian Zou<sup>1</sup><sup>,</sup><sup>*</sup></h4>
    </div>

    <div class="is-size-5 publication-authors">
<!--         <span class="author-block"><h4 style="text-align: center;"><b style="color:#d76100; font-weight:normal">&#x25B6</b>Peking University</h4><h4 style="text-align: center;"><b style="color:#d76100; font-weight:normal">&#x25B6 </b>ARC Lab, Tencent PCG</h4></span> -->
	<span class="author-block"><h4 style="text-align: center;"><b style="color:#d76100; font-weight:normal">&#x25B6 </b>1.&nbsp;Peking University</h4>
	<h4 style="text-align: center;"><b style="color:#d76100; font-weight:normal">&#x25B6 </b>2.&nbsp;ARC Lab, Tencent PCG</h4></span>
    </div>

    <br><br>


<!--     <div class="column has-text-centered">
        <span class="link-block">
            <a href="https://github.com/TencentARC-QQ/UT-CMVMR" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
                <i class="fab fa-github"></i>
            </span>
            <span>Github</span>
            </a>
        </span>

        <span class="link-block">
            <a href="http://arxiv.org/abs/2309.09421" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
                <i class="ai ai-arxiv"></i>
            </span>
            <span>Paper</span>
            </a>
        </span>
    </div> -->
	
</div>

    <div class="container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded">
    <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Abstraction</h2>
        </div>
      </div>

    <p> Speaker diarization in real-world videos presents significant challenges due to varying acoustic conditions, diverse scenes, and the presence of off-screen speakers, among other factors. This paper builds upon a previous study (AVR-Net) and introduces a novel multi-modal speaker diarization system, AFL-Net. Unlike AVR-Net, which independently extracts high-level representations from each modality, AFL-Net employs a multi-modal cross-attention mechanism. This approach generates high-level representations from each modality while conditioning on each other, ensuring a more comprehensive information fusion across modalities to enhance identity discrimination.
Furthermore, the proposed AFL-Net incorporates dynamic lip movement as an additional modality to aid in distinguishing each segment's identity. We also introduce a masking strategy during training that randomly obscures the face and lip movement modalities, which increases the influence of the audio modality on system outputs.
Experimental results demonstrate that our proposed model achieves state-of-the-art diarization error rates (DERs) of 23.65\% and 19.76\% on the AVA-AVD dataset when trained on the AVA-AVD dataset and a combined dataset of VoxCeleb1, VoxCeleb2, and AVA-AVD, respectively. 
These performance results represent a relative DER decrease of 13.8\% and 7.0\% compared to AVR-Net, respectively. Moreover, our experiments confirm the effectiveness of the proposed system, even under varying missing rates of visual features.  
    </p>
</div>


	

<div class="container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded">
    <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Method</h2>
        </div>
      </div>
 <p style="text-align: center;">  Codes will be available upon acceptance </p>
    <div class="text-center">
        <img id="afl" width="90%" src="demo/framework.PNG">
    </div>
</div>

	
<div class="container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded">
    <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Demo</h2>
        </div>
      </div>
    <table>
    <tr>
      <th scope="col" style="width: 33.33%">AFL-Net&nbsp;(ours)</th>
      <th scope="col" style="width: 33.33%">AVR-Net&nbsp;(baseline)</th>
      <th scope="col" style="width: 33.33%">Some&nbsp;analysis</th>
    </tr>
	    
    <tr>
      <td>
      <video-container>
        <video src="demo/1-2.mp4" controls></video>
      </video-container>
      </td>

      <td>
      <video-container>
        <video src="demo/1-1.mp4" controls></video>
      </video-container>
      </td>

      <td>In this video, there's a conversation between two people. 
	While both models correctly understand the first two sentences, the baseline model encounters a challenge in the third sentence, possibly due to the visual and auditory information 
	becoming unclear when the camera zooms out. AFL-Net, on the other hand, analyzes the sentence correctly, which may attribute to the multimodal information.</td>
    </tr>
	    
    <tr>
      <td>
      <video-container>
        <video src="demo/2-1.mp4" controls></video>
      </video-container>
      </td>

      <td>
      <video-container>
        <video src="demo/2-2.mp4" controls></video>
      </video-container>
      </td>

      <td>The audio and video conditions for this dialogue are considered optimal. However, owing to the speakers' closely resembling visual and pronunciation features, 
	      the baseline model might encounter challenges in distinguishing them, while AFL-Net potentially achieves accurate judgments, which may attribute to the advanced feature representation.</td>
    </tr>
	    
    <tr>
      <td>
          <video-container>
              <video src="demo/3-1.mp4" controls></video>
          </video-container>

      </td>
      <td>
          <video-container>
          <video src="demo/3-2.mp4" controls></video>
          </video-container>
      </td>

      <td>In this video, there is an occurrence of the speaker being off-screen. In our training process, 
      we emphasize the importance of audio features, probably enabling accurate judgments in many similar situations.</td>
    </tr>
  </table>


</div>


