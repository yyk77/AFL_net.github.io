
<!DOCTYPE html>
<body>

<head>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"
          integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <link href='http://fonts.googleapis.com/css?family=Lato:300,400,900' rel='stylesheet' type='text/css'>
    <link href="style.css" rel="stylesheet">
    <meta charset="utf-8">

	<title>AFL-Net</title>
	<link href="css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">

    <style>
        table {
      border-collapse: collapse;
      width: 100%;
    }

    td, th {
        border: 1px solid #777;
        text-align: center;
        padding: 8px;
        border-left:0px;
        border-right:0px;
    }

    .video-container {
      height: 100px; /* 设置视频容器的固定高度 */
      padding-bottom: 56.25%;
    }

    .video-container video {
      width: 100%;
      height: 100%;
    }


    .video-container .play-icon {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      font-size: 48px;
      color: #fff;
      cursor: pointer;
      opacity: 0.7;
    }

  </style>

    <script>
 const leftArrow = document.querySelector('.left-arrow');
const rightArrow = document.querySelector('.right-arrow');
const parentContainer = document.querySelector('.parent-container');

let currentIndex = 0;

leftArrow.addEventListener('click', scrollLeft);
rightArrow.addEventListener('click', scrollRight);

function scrollLeft() {
  if (currentIndex > 0) {
    currentIndex--;
    scrollToCurrentIndex();
  }
}

function scrollRight() {
  if (currentIndex < parentContainer.childElementCount - 1) {
    currentIndex++;
    scrollToCurrentIndex();
  }
}

function scrollToCurrentIndex() {
  const offset = currentIndex * parentContainer.offsetWidth;
  parentContainer.scrollTo({
    left: offset,
    behavior: 'smooth'
  });
}
        function playVideo(video) {
          video.play();
        }

        function pauseVideo(video) {
          video.pause();
        }
  </script>
</head>

<body data-new-gr-c-s-check-loaded="14.1091.0" data-gr-ext-installed="">


<div class="container" >
<header role="banner">
</header>
<main role="main">
<article itemscope itemtype="https://schema.org/BlogPosting">

<div class="container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded">
    <div class="column has-text-centered">
    <h1 class="title is-1 publication-title"> <b>AFL-Net:</b>  </h1>
    <h3 class="title is-3 publication-title"> INTEGRATING AUDIO, FACIAL, AND LIP MODALITIES WITH </h3>
    <h2 class="title is-3 publication-title"> CROSS-ATTENTION FOR ROBUST SPEAKER DIARIZATION IN THE WILD </h2>
    </div>
    <br>
    <div class="is-size-5">
        <span class="author-block">
<!--           <h4 style="text-align: center;"><b>Authors: </b>Yongkang Yin<sup>1</sup><sup>,</sup><sup>&dagger;</sup>, Xu Li<sup>2</sup><sup>,</sup><sup>*</sup>, Ying Shan<sup>2</sup>, Yuexian Zou<sup>1</sup><sup>,</sup><sup>*</sup></h4> -->
    </div>

    <div class="is-size-5 publication-authors">
<!--         <span class="author-block"><h4 style="text-align: center;"><b style="color:#d76100; font-weight:normal">&#x25B6</b>Peking University</h4><h4 style="text-align: center;"><b style="color:#d76100; font-weight:normal">&#x25B6 </b>ARC Lab, Tencent PCG</h4></span> -->
<!-- 	<span class="author-block"><h4 style="text-align: center;"><b style="color:#d76100; font-weight:normal">&#x25B6 </b>1. Peking University</h4>
	<h4 style="text-align: center;"><b style="color:#d76100; font-weight:normal">&#x25B6 </b>2. ARC Lab, Tencent PCG</h4>
	<br>
	<h5 style="text-align: center;">* Corresponding authors</h4>
	<h5 style="text-align: center;">&dagger; This work was done when Yongkang Yin was an intern at ARC Lab, Tencent PCG.</h4>
	</span> -->
    </div>

    <br><br>


<!--     <div class="column has-text-centered">
        <span class="link-block">
            <a href="https://github.com/TencentARC-QQ/UT-CMVMR" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
                <i class="fab fa-github"></i>
            </span>
            <span>Github</span>
            </a>
        </span>

        <span class="link-block">
            <a href="http://arxiv.org/abs/2309.09421" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
                <i class="ai ai-arxiv"></i>
            </span>
            <span>Paper</span>
            </a>
        </span>
    </div> -->
	
</div>

    <div class="container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded">
    <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Abstract</h2>
        </div>
      </div>

    <p> Speaker diarization in real-world videos presents significant challenges due to varying acoustic conditions, diverse scenes, and the presence of off-screen speakers, among other factors. This paper builds upon a previous study (AVR-Net) and introduces a novel multi-modal speaker diarization system, AFL-Net. Unlike AVR-Net, which independently extracts high-level representations from each modality, AFL-Net employs a multi-modal cross-attention mechanism. This approach generates high-level representations from each modality while conditioning on each other, ensuring a more comprehensive information fusion across modalities to enhance identity discrimination.
Furthermore, the proposed AFL-Net incorporates dynamic lip movement as an additional modality to aid in distinguishing each segment's identity. We also introduce a masking strategy during training that randomly obscures the face and lip movement modalities, which increases the influence of the audio modality on system outputs.
Experimental results demonstrate that our proposed model achieves state-of-the-art diarization error rates (DERs) of 23.65% and 19.76% on the AVA-AVD dataset when trained on the AVA-AVD dataset and a combined dataset of VoxCeleb1, VoxCeleb2, and AVA-AVD, respectively. 
These performance results represent a relative DER decrease of 13.8% and 7.0% compared to AVR-Net, respectively. Moreover, our experiments confirm the effectiveness of the proposed system, even under varying missing rates of visual features.  
    </p>
</div>


	

<div class="container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded">
    <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Method</h2>
        </div>
      </div>
    <div class="text-center">
	<p>  Codes will be available upon acceptance </p>
        <img id="afl" width="90%" src="demo/framework.PNG">
    </div>
</div>

	
<div class="container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded">
    <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Demo</h2>
        </div>
      </div>
    <table>
    <tr>
      <th scope="col" style="width: 33.33%">AFL-Net&nbsp;(ours)</th>
      <th scope="col" style="width: 33.33%">AVR-Net&nbsp;(baseline)</th>
      <th scope="col" style="width: 33.33%">Some&nbsp;analysis</th>
    </tr>
	    
    <tr>
      <td>
      <video-container>
        <video src="demo/1-1.mp4" controls></video>
      </video-container>
      </td>

      <td>
      <video-container>
        <video src="demo/1-2.mp4" controls></video>
      </video-container>
      </td>

      <td>In this video, a conversation unfolds between two individuals. Both models accurately identify the first two sentences. 
	However, the baseline model stumbles upon the third sentence, potentially due to the confusion arising from the simultaneous appearance of both individuals' facial information on the screen. 
	Conversely, AFL-Net correctly recognizes the sentence, a success that could be attributed to the incorporation of an additional lip movement modality.</td>
    </tr>
	    
    <tr>
      <td>
      <video-container>
        <video src="demo/2-1.mp4" controls></video>
      </video-container>
      </td>

      <td>
      <video-container>
        <video src="demo/2-2.mp4" controls></video>
      </video-container>
      </td>

      <td>In this video segment, the AVR-Net mistakenly identifies the second individual as the first throughout the entire segment, potentially due to their similar visual appearances. 
	However, the proposed AFL-Net accurately recognizes the second individual, thereby validating the effectiveness of fusion strategy among different modalities.</td>
    </tr>
	    
    <tr>
      <td>
          <video-container>
              <video src="demo/3-1.mp4" controls></video>
          </video-container>

      </td>
      <td>
          <video-container>
          <video src="demo/3-2.mp4" controls></video>
          </video-container>
      </td>

      <td>In this video, there's an instance where the second speaker is off-screen. 
	The AVR-Net incorrectly identifies this speaker as the first one, whereas the proposed AFL-Net accurately recognizes the speaker. 
	This could be because the proposed model is trained to rely more heavily on the audio modality, which is directly linked to the speaker's identity.</td>
    </tr>
  </table>


</div>


